import logging
import os
import sys
import pandas as pd
from os import listdir
from os.path import isfile, join, isdir
import argparse
import seaborn as sns
import matplotlib.pyplot as plt

logger = logging.getLogger(__name__)

parser = argparse.ArgumentParser(
                    prog='Generic search output analyser',
                    description='Analyse cm/..._result.csv file generated by problem analysis',
                    epilog='---')
parser.add_argument("-i", "--fileIndex", help="if passed program will use this value to look in cm directory instead of generating cin query",
                    type=str)

def ask_for_filepath(selection=None):
    dir_path = os.path.dirname(os.path.realpath(__file__))
    file_path = "/".join(dir_path.split("/")[:-1] + ["cm"])
    logger.info(dir_path)
    onlydirs = [f for f in listdir(file_path) if isdir(join(file_path, f))]
    onlydirs.sort()
    model_names = ["_".join(f[:-1]) if f[-1] == "validation" else g for f,g in zip(
        [f.split("_") for f in onlydirs], onlydirs)]

    if selection is not None:
        file = selection
    else:
        for i, name in enumerate(onlydirs):
            logger.info(f' => {i}. {name}')
        file = input("Enter file number (or enter multiple files using ','): ")

    if "," in file:
        file = [int(f) for f in file.split(",")]
    else:
        file = [int(file)]

    selected_model_names = [model_names[f].split("_")[-1] for f in file]
    if len(set(selected_model_names)) != len(selected_model_names):
        raise RuntimeError("ask_for_filepath: Cannot select same model for comparison")

    return [file_path + "/" + onlydirs[f] for f in file], selected_model_names

def plot(param_columns, df):
    fig, ax = plt.subplots(len(param_columns))
    fig.suptitle("vs Value")
    if len(param_columns) == 1:
        ax = [ax]

    for _ax, param in zip(ax, param_columns):
        sns.lineplot(data=df, x=param, y="value", ax=_ax)

    fig, ax = plt.subplots(len(param_columns))
    fig.suptitle("vs Time")
    if len(param_columns) == 1:
        ax = [ax]
    for _ax, param in zip(ax, param_columns):
        sns.lineplot(data=df, x=param, y="time_mean", ax=_ax)

def relative_loss(df):
    columns = [c for c in df.columns if "_Mean" in c]
    models = [c.replace("_Mean", "") for c in columns]
    for m1, c1 in zip(models, columns):
        for m2, c2 in zip(models, columns):
            if c1 != c2:
                df[f"{m1} vs. {m2} [%]"] = 100* (df[c2] - df[c1]) / df[c2]

    return df

def main(args):
    logging.basicConfig(level=logging.INFO)
    logger.info('Started')
    file_paths, models_name = ask_for_filepath(args.fileIndex)

    results_df = []
    for file_path in file_paths:
        df = pd.read_csv(os.path.join(file_path, "result.csv"), index_col=0)
        df.sort_values(by="value", inplace=True)
        param_columns = [col for col in df.columns if not any(v in col for v in ["value", "_std", "_mean"])]
        metric_columns_mean = [col for col in df.columns if "_mean" in col]
        metric_columns_std = [col for col in df.columns if "_std" in col]

        logger.info("Best model parametrisation\n")
        print(df[param_columns].iloc[0])
        logger.info("Result: ")

        result_df = pd.DataFrame({
            "Mean": df.iloc[0][metric_columns_mean].values,
            "Std": df.iloc[0][metric_columns_std].values
        }, index=pd.Index([m.replace("_mean", "") for m in metric_columns_mean], name="Metric"))
        # result_df["Mean"] = result_df["Mean"].map("{:,.3f}".format)
        # result_df["Std"] = result_df["Std"].map("{:,.4f}".format)
        results_df.append(result_df)

        # plots are important only during hyper parametrisation not during validation and model comparison
        # if not "validation" in file_path:
        #     plot(param_columns, file_paths)

    results_combined = pd.DataFrame({}, index=results_df[0].index)

    for model_name, res in zip(models_name, results_df):
        res.columns = [f"{model_name}_{c}" for c in res.columns]
        results_combined = results_combined.join(res)

    results_relative_loss = relative_loss(results_combined)
    print(results_relative_loss.to_string(float_format=lambda x: f"{x:.3f}"))
    plt.show()
    logger.info('Finished')

if __name__ == '__main__':
    args = parser.parse_args()
    main(args)